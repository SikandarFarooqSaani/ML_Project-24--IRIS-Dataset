# ğŸŒ¸ Iris Flower Classification with Voting Classifier  

![Iris Logo](https://img.icons8.com/emoji/96/cherry-blossom.png)  

---

## ğŸ“Œ Project Overview  
This project is all about predicting the type of **Iris flowers** ğŸŒº using **Machine Learning models**.  
We take the famous **Iris Dataset** from Kaggle and put multiple classifiers to the test.  
Finally, we combine them using a **Voting Classifier** to see how teamwork (models working together ğŸ¤) can sometimes outperform individual models.  

---

## ğŸ“‚ Dataset  
ğŸ“Š **Dataset link:** [Iris Dataset on Kaggle](https://www.kaggle.com/datasets/saurabh00007/iriscsv)  

- No null or duplicate values âœ…  
- Removed unnecessary columns like **id** ğŸ—‘ï¸  
- Encoded the target labels using **LabelEncoder** ğŸ¯  

---

## âš™ï¸ Tech Stack & Libraries  
- **Python** ğŸ  
- **OS, NumPy, Pandas** â†’ for handling data  
- **Seaborn & Matplotlib** â†’ for beautiful visualizations ğŸ“Š  
- **Scikit-learn** â†’ ML models and evaluation  

---

## ğŸ–¼ï¸ Data Visualization  
We created a **pairplot** for all features to see how the classes separate visually:  

<img width="1059" height="986" alt="24-1" src="https://github.com/user-attachments/assets/72350cf2-1ba8-4a4c-95a1-07c18786eb89" />
 

---

## ğŸš€ Project Workflow  

1. **Preprocessing**  
   - Dropped unnecessary columns (like `id`)  
   - Encoded target labels using **LabelEncoder**  

2. **Feature Selection**  
   - Extracted only **2 feature columns** to make prediction harder (more challenging task ğŸ¯).  

3. **Train-Test Split**  
   - Divided data into `X` (features) and `y` (target)  
   - Applied **train_test_split**  

4. **Base Models Used**  
   - Logistic Regression  
   - Random Forest  
   - K-Nearest Neighbors (KNN)  

5. **Individual Model Performance (Cross-Val Score)**  
   - Logistic Regression â†’ **0.75**  
   - Random Forest â†’ **0.62**  
   - KNN â†’ **0.61**  

6. **Voting Classifier (Logistic + RF + KNN)**  
   - Hard Voting â†’ **0.67**  
   - Soft Voting â†’ **0.67**  
   - Tried **nested loops for weights** â†’ Best score was **0.70** with weights:  
     - Logistic Regression = 3  
     - Random Forest = 1  
     - KNN = 2  

7. **Support Vector Machines (SVMs)**  
   - Trained **5 different SVMs** with polynomial degrees from 1 â†’ 5  
   - Results:  
     - Degree 1 â†’ 85%  
     - Degree 2 â†’ 85%  
     - Degree 3 â†’ 89%  
     - Degree 4 â†’ 81%  
     - Degree 5 â†’ 86%  

8. **Voting on SVMs**  
   - Combined all SVMs in a **Voting Classifier**  
   - Achieved an impressive **0.93 Cross-Val Score** ğŸ‰  

---

## ğŸ“Š Results  

| Model                  | Accuracy |
|-------------------------|----------|
| Logistic Regression     | 0.75     |
| Random Forest           | 0.62     |
| KNN                     | 0.61     |
| Voting (LR + RF + KNN)  | 0.67     |
| Weighted Voting (3-1-2) | 0.70     |
| Best SVM (deg=3)        | 0.89     |
| Voting (All SVMs)       | **0.93** âœ… |

---

## ğŸ’¡ Key Takeaways  
- Ensemble models can sometimes improve accuracy, but not always ğŸš¦  
- SVM with degree 3 performed the best among single models ğŸ†  
- Voting multiple SVMs together gave the **highest accuracy (0.93)** ğŸ¯  

---

### ğŸ“Œ Future Improvements

Try Gradient Boosting / XGBoost for even better performance ğŸŒŸ

Add hyperparameter tuning with GridSearchCV / RandomizedSearchCV ğŸ”§

Use all 4 features instead of just 2 for full dataset exploration ğŸŒ¸

### ğŸ™Œ Acknowledgements

Kaggle Dataset

Scikit-learn Team for easy ML model implementation

Seaborn & Matplotlib for beautiful visualizations
